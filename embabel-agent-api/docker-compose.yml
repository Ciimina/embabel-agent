services:
  llama3.2:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_MODELS=llama3.2
  qwen3:
    image: ollama/ollama:latest
    ports:
      - "11435:11434"
    environment:
      - OLLAMA_MODELS=qwen2.5

  zipkin:
    image: 'openzipkin/zipkin:latest'
    ports:
      - "9411:9411"

  # the environment variables that will be automatically set in this process are
  # LLAMA3.2_URL - this is the http base url - add /chat/completions to this for doing completions
  # LLAMA3.2_MODEL - this is the name of the model that should be passed in the request
