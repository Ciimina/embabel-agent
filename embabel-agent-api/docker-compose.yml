services:

  llama3.2:
    provider:
      type: model
      options:
        model: ai/llama3.2

  qwen3:
    provider:
      type: model
      options:
        model: ai/qwen3


  postgres:
    image: pgvector/pgvector:pg15
    ports:
      - "5432:5432"
    volumes:
      - ~/apps/postgres:/var/lib/postgresql/data
      - ./init.sql:/docker-entrypoint-initdb.d
    environment:
      - POSTGRES_PASSWORD=/${PGPASSWORD:-look_to_the_stars}
      - POSTGRES_USER=${PGUSER:-astrid}
      - POSTGRES_DB=embabel

  zipkin:
    image: 'openzipkin/zipkin:latest'
    ports:
      - "9411:9411"


  # the environment variables that will be automatically set in this process are
  # LLAMA3.2_URL - this is the http base url - add /chat/completions to this for doing completions
  # LLAMA3.2_MODEL - this is the name of the model that should be passed in the request
  # MCP_GATEWAY_ENDPOINT - this is a listening socket - connect to it and use pass the out/in streams to
  #                        an MCP STDIO client
#  star-news-agent:
#    image: springrod/embabel-agent-api:1.0.0-SNAPSHOT
#    depends_on:
#      - llama3.2
#      - mcp-gateway

