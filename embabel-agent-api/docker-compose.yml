services:

  llama3.2:
    provider:
      type: model
      options:
        model: ai/llama3.2

  qwen3:
    provider:
      type: model
      options:
        model: ai/qwen3


  neo4j:
    image: neo4j:5.26
    ports:
      - "7474:7474"
      - "7687:7687"
      - "7473:7473"
    environment:
      - NEO4J_AUTH=${NEO4J_USERNAME:-neo4j}/${NEO4J_PASSWORD:-brahmsian}
      - NEO4J_PLUGINS=["apoc"]


  zipkin:
    image: 'openzipkin/zipkin:latest'
    ports:
      - "9411:9411"


  # the environment variables that will be automatically set in this process are
  # LLAMA3.2_URL - this is the http base url - add /chat/completions to this for doing completions
  # LLAMA3.2_MODEL - this is the name of the model that should be passed in the request
  # MCP_GATEWAY_ENDPOINT - this is a listening socket - connect to it and use pass the out/in streams to
  #                        an MCP STDIO client
#  star-news-agent:
#    image: springrod/embabel-agent-api:1.0.0-SNAPSHOT
#    depends_on:
#      - llama3.2
#      - mcp-gateway

